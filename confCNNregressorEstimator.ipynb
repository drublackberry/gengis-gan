{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import yaml\n",
    "from abc import abstractmethod\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module definitions\n",
    "class TensorDict(dict):\n",
    "    \"\"\"\n",
    "    Class that contains a dictionary of tensors\n",
    "    \"\"\"\n",
    "    def last(self):\n",
    "        return self.get(list(self.keys())[-1])\n",
    "    \n",
    "    def get_many(self, t_list):\n",
    "        return {x: self.get(x) for x in t_list}\n",
    "\n",
    "activation_dict = {\"relu\": tf.nn.relu, None: None, \"None\": None}\n",
    "\n",
    "def create_conv_layer(tensor_in, spec, name, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    return {name: tf.layers.conv2d(inputs=tensor_in, \n",
    "                                   filters=spec['filters'],\n",
    "                                   kernel_size=spec['kernel_size'],\n",
    "                                   padding=spec['padding'],\n",
    "                                   activation=activation_dict[spec['activation']],\n",
    "                                   name=name)}\n",
    "\n",
    "def create_maxpool_layer(tensor_in, spec, name, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    return {name: tf.layers.max_pooling2d(inputs=tensor_in,\n",
    "                                          pool_size=spec['pool_size'],\n",
    "                                          strides=spec['strides'],\n",
    "                                          name=name)}\n",
    "\n",
    "def create_dense_layer(tensor_in, spec, name, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    # creates an extra layer that must be flattened as well\n",
    "    out = {}\n",
    "    out[name+'_flatten'] = tf.reshape(tensor_in, [-1, np.prod(tensor_in.shape[1:4])])\n",
    "    out[name] = tf.layers.dense(inputs=out[name+'_flatten'],\n",
    "                                units=spec['units'],\n",
    "                                activation=activation_dict[spec['activation']],\n",
    "                                name=name)\n",
    "    return out\n",
    "                      \n",
    "def create_dropout_layer(tensor_in, spec, name, mode=tf.estimator.ModeKeys.TRAIN):\n",
    "    return {name: tf.layers.dropout(inputs=tensor_in, rate=spec['rate'], training = mode == tf.estimator.ModeKeys.TRAIN) }\n",
    "\n",
    "l_dict = {'CONV':create_conv_layer, 'POOL':create_maxpool_layer, 'FC': create_dense_layer, 'DROP': create_dropout_layer}\n",
    "\n",
    "def tensorboard_summaries(conv_layers=None, dense_layers=None):\n",
    "    if conv_layers is not None:\n",
    "        for cl_name, cl in conv_layers.items():\n",
    "            shape = cl.shape.as_list()\n",
    "            for f in range(shape[3]):\n",
    "                tf.summary.image(\"{}/filter/{}\".format(cl_name, f), tf.reshape(cl[:, :, :, f], [tf.shape(cl)[0], shape[1], shape[2], 1]))  \n",
    "    if dense_layers is not None:\n",
    "        for dl_name, dl in dense_layers.items():\n",
    "           tf.summary.histogram(\"{}/weights\".format(dl_name), dl) \n",
    "\n",
    "        \n",
    "class confEstimator(tf.estimator.Estimator):\n",
    "    \"\"\"\n",
    "    Class that allows building a certain neural-net based on a configuration YAML file. It must be overcharged.\n",
    "    \"\"\"\n",
    "            \n",
    "    def __init__(self, conf_file, model_dir):\n",
    "        self.conf_file = conf_file\n",
    "        with open(conf_file) as f:\n",
    "            self.conf = yaml.load(f)\n",
    "        super(confEstimator, self).__init__(self.conf_model_fn, model_dir=model_dir)\n",
    "        \n",
    "    @abstractmethod\n",
    "    def conf_model_fn(self, features, labels, model):\n",
    "        pass\n",
    "    \n",
    "    def _get_tensor_dict(self, data_in, mode):\n",
    "        tensor_dict = TensorDict()  # will contain all the information on the layers and tensors\n",
    "        num_channels = 1 if len(data_in.shape)==3 else data_in.shape[3]\n",
    "        tensor_dict['input_layer'] = tf.reshape(data_in, [-1, data_in.shape[1], data_in.shape[2], num_channels]) \n",
    "        for l_name, l_def in self.conf['layers'].items():\n",
    "            tensor_dict.update(l_dict[l_def['type']](tensor_dict.last(), l_def['specs'], l_name, mode))\n",
    "        return tensor_dict\n",
    "    \n",
    "    def get_layers_of_type(self, l_type):\n",
    "        return [l_name for l_name, l_def in self.conf['layers'].items() if l_def['type']==l_type]\n",
    "    \n",
    "\n",
    "class confRegressorEstimator(confEstimator):\n",
    "    \"\"\"\n",
    "    Class that allows for a image classification based on a convolutional neural-net which architecture is on a YAML file\n",
    "    \"\"\"\n",
    "    def conf_model_fn(self, features, labels, mode):\n",
    "\n",
    "        # tensor_dict\n",
    "        td =  self._get_tensor_dict(features, mode)\n",
    "        \n",
    "        # tensorboard summary for weights and variables in the layers   \n",
    "        tensorboard_summaries(conv_layers=td.get_many(self.get_layers_of_type('CONV')), \n",
    "                             dense_layers=td.get_many(self.get_layers_of_type('FC')))\n",
    "\n",
    "        # predictions\n",
    "        predictions = {\n",
    "            \"predictions\": td.last()  # return the last predicted image\n",
    "        }\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            # wrap predictions into a class and return EstimatorSpec object\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "        \n",
    "        # minimize on cross-entropy\n",
    "        loss = tf.losses.mean_squared_error(labels=labels, predictions=td.last())  # loss is a scalar tensor\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.conf['learning_rate'])\n",
    "            train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "\n",
    "        eval_metric_ops = {\n",
    "            \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"predictions\"]),\n",
    "            \"mean_absolute_error\": tf.metrics.mean_absolute_error(labels=labels, predictionns=predictions[\"predictions\"]), \n",
    "            \"RMS\": tf.metrics.root_mean_squared_error(labels=labels, predictions=predictions[\"predictions\"])\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "sys.path.append(\"/jup/projects/mtgtools2/\")  # ensure we find the MTG libraries\n",
    "import pymtg.fci.esl.training_data as esl\n",
    "import pymtg\n",
    "\n",
    "d = datetime.datetime.now()\n",
    "RETINAS_JSON = Path(pymtg.__file__).parent.parent / \"test/FCI_ESL/pyesl/config/retinas.json\"\n",
    "ESL_PATH = Path(\"/mnt/data/FCI/ESL/nc/output/20170615T152646/ch123/\")\n",
    "FCI_PATH = Path(\"/mnt/data/FCI/ESL/nc/swaths/\")\n",
    "train_size = 100\n",
    "test_size = 20\n",
    "\n",
    "files = esl.prepare_training_data(FCI_PATH, ESL_PATH, 0, str(RETINAS_JSON))\n",
    "train_batch = list(files[\"train\"].batches(\"ch123\", train_size, 20000, num_batches=1))\n",
    "train_x =train_batch[0][0]\n",
    "train_y = train_batch[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = list(files[\"test\"].batches(\"ch123\", test_size, 20000, num_batches=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_batch[0][0]\n",
    "test_y = test_batch[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the estimator\n",
    "esl_regressor = confRegressorEstimator('example_config.yml', \n",
    "                                       model_dir=\"/mnt/data/tensorboard/esl_conf_fc_model_{}\".format(d.strftime(\"%Y%M%dT%H%m%S\")))\n",
    "\n",
    "# train\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x=train_x, \n",
    "    y=train_y,\n",
    "    batch_size=100,\n",
    "    num_epochs=None,\n",
    "    shuffle=True)\n",
    "esl_regressor.train(input_fn=train_input_fn, steps=200)\n",
    "\n",
    "# evaluate\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x=test_x,\n",
    "    y=test_y,\n",
    "    num_epochs=1, \n",
    "    shuffle=False)\n",
    "eval_results = esl_regressor.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 113)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
