{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESL estimation with GAN\n",
    "\n",
    "This notebook allows to train a GAN network to synthesize stray-light images based on network architectures specified via YAML configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from configuredEstimator import TensorDict\n",
    "from pathlib import Path\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "sys.path.append(\"/jup/projects/mtgtools2/\")  # ensure we find the MTG libraries\n",
    "import pymtg.fci.esl.training_data as esl\n",
    "import pymtg\n",
    "\n",
    "MNT_DIR = Path(\"/scratch/andreu/\")\n",
    "RETINAS_JSON = Path(pymtg.__file__).parent.parent / \"test/FCI_ESL/pyesl/config/retinas.json\"\n",
    "ESL_PATH = MNT_DIR / \"ESL/nc/output/20170615T152646/ch123/\"\n",
    "FCI_PATH = MNT_DIR / \"ESL/nc/swaths/\"\n",
    "\n",
    "def collapse_scene(scene, step=1):\n",
    "    col_scene = scene[0]\n",
    "    for s in scene[1:]:\n",
    "        col_scene = np.insert(col_scene, -step, s.transpose()[-step], axis=1)  # create single input scene.\n",
    "    return col_scene\n",
    "\n",
    "def plot_scene(scene):\n",
    "    plt.imshow(collapse_scene(scene))\n",
    "    plt.title('Input scene')\n",
    "    plt.show()\n",
    "\n",
    "def plot_esl(esl, title='ESL'):\n",
    "    plt.imshow(esl.T)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def get_model_name(name='Unnamed_conf_model'):\n",
    "    d = datetime.datetime.now()\n",
    "    return \"{}_{}\".format(name, d.strftime(\"%Y%m%dT%H%M%S\"))\n",
    "                          \n",
    "def get_model_dir(model_dir, **kwargs):\n",
    "    return \"{}/{}\".format(model_dir, get_model_name(**kwargs))\n",
    "                          \n",
    "def input_fn_np(size, train_or_test='train', collapsed=False, num_batches=1, mode=tf.estimator.ModeKeys.TRAIN, plot_data=False, verbose=False):\n",
    "    \"\"\"\n",
    "    Input function that serves data to the estimator.\n",
    "    If not collapsed it will return x={features:(size*num_batches, 129, 186), y=(size*num_batches, 113).\n",
    "    If collapsed, it will return x={features:(num_batches, 129, 186+size)}, y=(num_batches, 113xsize)\n",
    "    The argument train_or_test should be set to train for the moment, as test is not validated.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Calling input_fn with collapsed={}, num_batches={}, size={}\".format(collapsed, num_batches, size))\n",
    "    files = esl.prepare_training_data(FCI_PATH, ESL_PATH, 0, str(RETINAS_JSON))\n",
    "    out_batch = list(files[train_or_test].batches(\"ch123\", size, 20000, num_batches=num_batches))  # use always 'train', 'test' is not validated\n",
    "    if collapsed:\n",
    "        features = np.array([collapse_scene(out_batch[i][0]) for i in range(num_batches)])\n",
    "        labels = np.array([out_batch[i][1].flatten() for i in range(num_batches)])\n",
    "    else:\n",
    "        features = np.concatenate([out_batch[i][0] for i in range(len(out_batch))], axis=0)  # flatten into a (num_batches*size, 129, 113)\n",
    "        labels = np.concatenate([out_batch[i][1] for i in range(len(out_batch))], axis=0)\n",
    "    if plot_data:  # for validation purposes during prediction, will plot only the first image\n",
    "        if collapsed:\n",
    "            for f in features:\n",
    "                plt.imshow(f)\n",
    "                plt.show()\n",
    "            for l in labels:\n",
    "                plt.imshow(l.reshape(out_batch[0][1].shape))\n",
    "                plt.show()\n",
    "        else:\n",
    "            plot_scene(features)\n",
    "            plot_esl(labels)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Graph definition\n",
    "\n",
    "Since this network is a Generative Adversarial Network (GAN), the graph is composed by two main elements:\n",
    "* A **generator** network, which function is to create synthesized stray-light images from earth images.\n",
    "* A **discriminator** network, which function is to distinguish apart the synthesized stray-light images from the real stray-light images.\n",
    "\n",
    "**NOTE**: the GAN model works only using the non-collapsed mode for the ESL batches. The collapsed mode is not supported, as it has anyway given poorer results.\n",
    "\n",
    "The flow is controlled via the following parameters:\n",
    "\n",
    "* SIZE is the size of the ESL images (SIZE x NUM_BATCHES, 113).\n",
    "* TRAIN_STEPS is the number of training iterations.\n",
    "* TEST_STEPS is the number of test iterations, that will be plotted after the training has finalized.\n",
    "* MODEL_GEN is the name of the YAML configuration file that defines the architecture of the generator.\n",
    "* MODEL_DIS is the name of the YAML configuration file that defines the architecture of the discriminator.\n",
    "* NUM_BATCHES is the number of batches that are fed at every iteration. It is redundant with the SIZE parameters as the total size of the ESL is (SIZE x NUM_BATCHES, 113). It is advised to left it set to 1.\n",
    "* VERBOSE, if set to True the process will inform about the shapes of the tensors created during graph definition.\n",
    "* PRINT_ITER controls the frequency of the messages displaying the loss of generator and discriminator. It will print a message every PRINT_ITER training steps.\n",
    "* SAVE_N_CHKPT controls the total number of checkpoints created. A final one is always created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 100\n",
    "TRAIN_STEPS = 100\n",
    "TEST_STEPS = 2  # number of test images to predict\n",
    "MODEL_GEN = 'CONV3.yml'\n",
    "MODEL_DIS = 'DISC_GAN.yml'\n",
    "NUM_BATCHES = 1\n",
    "VERBOSE = False\n",
    "PRINT_ITER = 2  # print loss every N iterations\n",
    "SAVE_N_CHKPT = 5  # how many checkpoints to save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving summaries and checkpoints on /scratch/tensorboard/GEN_CONV3.yml_DIS_DISC_GAN.yml_not_collapsed_size_100_batches_1_train_steps_100_20180823T143140\n",
      "Run 0, Generator loss = 0.0009325678693130612, Discriminator loss = 1.3862943649291992\n",
      "Run 2, Generator loss = 0.001425484661012888, Discriminator loss = 1.3862943649291992\n",
      "Run 4, Generator loss = 0.0015025836182758212, Discriminator loss = 1.3862943649291992\n",
      "Run 6, Generator loss = 8.599522698204964e-05, Discriminator loss = 1.3862943649291992\n",
      "Run 8, Generator loss = 0.0016339042922481894, Discriminator loss = 1.3862943649291992\n",
      "Run 10, Generator loss = 0.001478342222981155, Discriminator loss = 1.3862943649291992\n",
      "Run 12, Generator loss = 0.0008007189608179033, Discriminator loss = 1.3862943649291992\n",
      "Run 14, Generator loss = 0.000669955275952816, Discriminator loss = 1.3862943649291992\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "COLLAPSED = False\n",
    "\n",
    "def get_gan_config_name():\n",
    "    coll = 'collapsed' if COLLAPSED else 'not_collapsed'\n",
    "    return 'GEN_' + MODEL_GEN + '_DIS_' + MODEL_DIS + '_' + coll + '_size_' + str(SIZE) + '_batches_' + str(NUM_BATCHES) + '_train_steps_' + str(TRAIN_STEPS)\n",
    "\n",
    "def get_model_name(name='Unnamed_conf_model'):\n",
    "    d = datetime.datetime.now()\n",
    "    return \"{}_{}\".format(name, d.strftime(\"%Y%m%dT%H%M%S\"))\n",
    "                          \n",
    "def get_model_dir(model_dir, **kwargs):\n",
    "    return Path(model_dir) / get_model_name(**kwargs)\n",
    "\n",
    "def generator(z, reuse=False, verbose=False):\n",
    "    td = TensorDict(MODEL_GEN, z, prefix='GEN', batch_dim=0, create_scope=True, verbose=verbose)\n",
    "    return td.last()\n",
    "\n",
    "def discriminator(x, reuse=False, verbose=False):    \n",
    "    td = TensorDict(MODEL_DIS, x, prefix='DISC', batch_dim=None, create_scope=True, reuse=reuse, verbose=verbose)\n",
    "    return td.last()\n",
    "\n",
    "earth_images, esl_images = input_fn_np(SIZE, num_batches=NUM_BATCHES, plot_data=False, collapsed=COLLAPSED)\n",
    "X = tf.placeholder(tf.float32, esl_images.shape)\n",
    "Z = tf.placeholder(tf.float32, earth_images.shape)\n",
    "G_sample = generator(Z, verbose=VERBOSE)  # fake samples\n",
    "r_logits = discriminator(X, verbose=VERBOSE) # return the variable scope to be reused\n",
    "f_logits = discriminator(G_sample, reuse=True, verbose=VERBOSE)  # reuse is true\n",
    "\n",
    "# loss functions\n",
    "# discriminator loss is log(D(x)) + log(1-D(G(z)))\n",
    "disc_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=r_logits, labels=tf.ones_like(r_logits)) +\n",
    "                           tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits, labels=tf.zeros_like(f_logits)))\n",
    "\n",
    "# generator loss is the MSE\n",
    "gen_loss = tf.reduce_mean(tf.losses.mean_squared_error(labels=X, predictions=G_sample))\n",
    "# generator loss is log(G(z))\n",
    "#gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=f_logits, labels=tf.ones_like(f_logits)))\n",
    "\n",
    "# optimizers\n",
    "# we collect the variables to be updated using scope and var_list\n",
    "gen_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"GEN\")\n",
    "gen_step = tf.train.RMSPropOptimizer(learning_rate=0.0001).minimize(gen_loss, var_list=gen_vars)  # G train step\n",
    "disc_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DISC\")\n",
    "disc_step = tf.train.RMSPropOptimizer(learning_rate=0.0001).minimize(disc_loss, var_list=disc_vars)  # D train step\n",
    "\n",
    "# summary prep\n",
    "tf.summary.scalar('disc_loss', disc_loss)\n",
    "tf.summary.scalar('gen_loss', gen_loss)\n",
    "merged = tf.summary.merge_all()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# run session\n",
    "model_dir = get_model_dir(\"/scratch/tensorboard\", name=get_gan_config_name())\n",
    "chkpt_dir = model_dir / 'saved_checkpoints'\n",
    "print(\"Saving summaries and checkpoints on {}\".format(model_dir))\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run(session=sess)\n",
    "    train_writer = tf.summary.FileWriter(str(model_dir), sess.graph)\n",
    "    for i in range(TRAIN_STEPS):\n",
    "        Z_batch, X_batch = input_fn_np(SIZE, num_batches=NUM_BATCHES, plot_data=False, collapsed=COLLAPSED)\n",
    "        _, dloss, summary = sess.run([disc_step, disc_loss, merged], feed_dict={X:X_batch, Z:Z_batch})\n",
    "        _, gloss = sess.run([gen_step,gen_loss], feed_dict={Z: Z_batch, X: X_batch})\n",
    "        train_writer.add_summary(summary, i)\n",
    "        if i % PRINT_ITER == 0:        \n",
    "            print(\"Run {}, Generator loss = {}, Discriminator loss = {}\".format(i, gloss, dloss))\n",
    "        if i % (TRAIN_STEPS/SAVE_N_CHKPT) == 0:\n",
    "            save_path = saver.save(sess, str(chkpt_dir / '{}.ckpt'.format(i)))            \n",
    "    save_path = saver.save(sess, str(chkpt_dir / 'final.ckpt'))  \n",
    "    \n",
    "    # prediction test\n",
    "    for j in range(TEST_STEPS):\n",
    "        Z_batch, X_batch = input_fn_np(SIZE, num_batches=NUM_BATCHES, plot_data=False, collapsed=COLLAPSED)\n",
    "        esl_pred =  sess.run(G_sample, feed_dict={Z:Z_batch})\n",
    "        plot_scene(Z_batch)\n",
    "        plot_esl(X_batch, title='Real ESL')\n",
    "        plot_esl(esl_pred, title='Predicted ESL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
